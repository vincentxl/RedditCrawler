{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw #Reddit Python wrapper\n",
    "import re #regex\n",
    "import pandas as pd #Data manipulation\n",
    "\n",
    "import time #Time\n",
    "import datetime as dt #Time\n",
    "\n",
    "import networkx as nx #Network analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get /r/ subreddits from subreddit description string using regex\n",
    "def getSubs(sub):\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    sub_description = subreddit.description\n",
    "    subsList = re.findall(r'/r/\\w+', sub_description)\n",
    "    subsList = [x[1:] for x in subsList]\n",
    "    return subsList\n",
    "\n",
    "#Return sfw-only list of subreddits\n",
    "def get_sfw_list(inp_list):\n",
    "    not_over18_list = []\n",
    "    \n",
    "    global checkedList\n",
    "    global errorList\n",
    "    global sfwList\n",
    "    \n",
    "    for x in inp_list:\n",
    "        if x not in checkedList:\n",
    "            checkedList.append(x)\n",
    "            try:\n",
    "                sub = reddit.subreddit(x[2:])\n",
    "                if sub.over18 == True: \n",
    "                    continue\n",
    "                else:\n",
    "                    not_over18_list.append(x)\n",
    "                    sfwList.append(x)\n",
    "            except:\n",
    "                errorList.append(x)\n",
    "        elif x in sfwList:\n",
    "            not_over18_list.append(x)\n",
    "        else:\n",
    "            continue\n",
    "    return not_over18_list\n",
    "\n",
    "#Return sorted lower-cased list of subreddits\n",
    "def lower_sort(inp_list):\n",
    "    out_list = [x.lower() for x in inp_list]\n",
    "    out_list.sort()\n",
    "    return out_list\n",
    "\n",
    "#Return display names of subreddits\n",
    "def get_display_name(inp_list):\n",
    "    out_list = []\n",
    "    for x in inp_list:\n",
    "        sub = reddit.subreddit(x[2:])\n",
    "        display = sub.display_name_prefixed\n",
    "        out_list.append(display)\n",
    "    return out_list\n",
    "\n",
    "#Add count of subreddits to a dictionary\n",
    "def addDict(dictionary, list_to_add):\n",
    "    for x in list_to_add:\n",
    "        if x in dictionary:\n",
    "            dictionary[x] = dictionary[x] + 1\n",
    "        else: \n",
    "            dictionary[x] = 1\n",
    "\n",
    "#Return list of tuples of linked subreddit pairs\n",
    "def createLinks(sub,subsList):\n",
    "    linkList = []\n",
    "    for x in subsList:\n",
    "        linkList.append((x,sub))\n",
    "    return linkList\n",
    "\n",
    "#Return list of tuples of linked subreddit pairs extending out from selected subreddit\n",
    "def extendOut(tuplesList, sub, layer):\n",
    "    newList = []\n",
    "    i = 0\n",
    "    searchList = ['r/'+sub]\n",
    "    while i < layer:\n",
    "        for x in tuplesList:\n",
    "            if x[1] in searchList:\n",
    "                newList.append(x)\n",
    "                searchList.append(x[0])\n",
    "            elif x[0] in searchList:\n",
    "                newList.append(x)\n",
    "                searchList.append(x[1])\n",
    "            else:\n",
    "                continue\n",
    "        i=i+1\n",
    "    return newList\n",
    "\n",
    "#Crawl reddit from an intial sub, returning list of tuples of subreddit pairs and dictionary of subreddit counts \n",
    "def crawlReddit(initial_sub, limit):\n",
    "    \n",
    "    #Initial subs list\n",
    "    subsList = get_sfw_list(lower_sort(getSubs(initial_sub)))\n",
    "    sub_dict = {}\n",
    "    listoflinks = [createLinks('r/'+initial_sub,subsList)]\n",
    "    \n",
    "    #Add initial subs list to dictionary\n",
    "    addDict(sub_dict,subsList)\n",
    "    \n",
    "    #Create search list from dictionary\n",
    "    searchList = list(sub_dict)\n",
    "    searchedList = ['r/'+initial_sub]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    i = 0 \n",
    "    \n",
    "    while limit > len(sub_dict):\n",
    "        for x in searchList:\n",
    "            if len(sub_dict) >= limit:\n",
    "                break\n",
    "            elif x not in searchedList: \n",
    "                try:\n",
    "\n",
    "                    #get subs, lowercase sort, get sfw list\n",
    "                    subsList = get_sfw_list(lower_sort(getSubs(x[2:])))\n",
    "                    addDict(sub_dict,subsList)     \n",
    "                    listoflinks.append(createLinks(x,subsList))\n",
    "\n",
    "                    searchList = list(set(searchList+subsList))\n",
    "                    searchedList.append(x)\n",
    "                    searchList = list(set(searchList)-set(searchedList))\n",
    "\n",
    "                except:\n",
    "                    errorList.append(x)\n",
    "                    searchList = list(set(searchList)-set(errorList))\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            end_time = time.time()\n",
    "            time_past =  dt.timedelta(seconds=round(end_time-start_time))\n",
    "            if len(searchedList) == 0: time_per_search = dt.timedelta(seconds=round(0)) \n",
    "            else: time_per_search = time_past/len(searchedList)\n",
    "            time_to_go = len(searchList)*time_per_search\n",
    "            est_total_time = time_past+time_to_go\n",
    "\n",
    "            print('Search: '+str(len(searchList)),'Searched: '+str(len(searchedList)),\n",
    "                  'Error: '+str(len(errorList)),'Checked: '+str(len(checkedList)),\n",
    "                  'SFW: '+str(len(sfwList)),'Sub_dict: '+str(len(sub_dict)),'Itter: '+str(i),\n",
    "                  'Time: '+str(time_past), 'Time-per-search: '+str(time_per_search),\n",
    "                  'Time-to-go: '+str(time_to_go), 'Total Time: '+str(est_total_time), end=\"\\r\") \n",
    "        i = i+1\n",
    "        \n",
    "    return listoflinks, sub_dict\n",
    "\n",
    "#Returns flattened list of tuples without self-referencing subreddits\n",
    "def flatten_tuples_list(inp_list):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    flat_list = flatten(inp_list)\n",
    "    #Get rid of self-referencing subs\n",
    "    unique_flat_list = []\n",
    "    for x in flat_list:\n",
    "        if x[0] != x[1]:\n",
    "            unique_flat_list.append(x)\n",
    "    return unique_flat_list\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Reddit - return list of subreddit links (edges) and dictionary of subreddit counts (nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access Reddit\n",
    "client_secret = ''\n",
    "client_id = ''\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     user_agent='crawling subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define lists\n",
    "subsList = []\n",
    "searchList = []\n",
    "searchedList = []\n",
    "errorList = []\n",
    "checkedList = []\n",
    "sfwList = []\n",
    "listoflinks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: 788 Searched: 215 Error: 17 Checked: 1021 SFW: 1003 Sub_dict: 1003 Itter: 3 Time: 0:02:59 Time-per-search: 0:00:00.832558 Time-to-go: 0:10:56.055704 Total Time: 0:13:55.055704\r"
     ]
    }
   ],
   "source": [
    "#Crawl Reddit starting from r/datascience until after 1000 subs limit passed\n",
    "#Return list of subreddit links as list of tuples\n",
    "#Return dictionary of subreddit counts\n",
    "listoflinks, sub_dict = crawlReddit('datascience',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gexf (Graphic Exchange XML Format) file for use in Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten list of subreddit links\n",
    "flattened_tuples_list = flatten_tuples_list(listoflinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary of new flattened list of tuples\n",
    "tuple_dict = {}\n",
    "list_for_dict = []\n",
    "for x in flattened_tuples_list:\n",
    "    list_for_dict.append(x[0])\n",
    "        \n",
    "addDict(tuple_dict,list_for_dict)\n",
    "\n",
    "tuple_dict = list(tuple_dict.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create networkx Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#Add nodes to networkx Graph from new dictionary\n",
    "for x in tuple_dict:\n",
    "    G.add_node(x[0],weight=x[1])\n",
    "    \n",
    "#Add edges to networkx Graph from flattened list of subreddit links\n",
    "G.add_edges_from(flattened_tuples_list)\n",
    "\n",
    "#Write gexf file (Graph Exchange XML Format) for use in Gephi\n",
    "nx.write_gexf(G, 'datascience.gexf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
